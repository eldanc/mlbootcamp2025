{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk0LoqhRIhuT"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2025/blob/main/lab_3_1_neuralnets.ipynb)\n",
        "\n",
        "# UofT FASE ML Bootcamp\n",
        "#### Wednesday June 11, 2025\n",
        "#### Intro to Neural Networks in PyTorch - Lab 1, Day 3\n",
        "#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya\n",
        "##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca, edited by Jake Mosseri and Nakul Upadhya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWWYgttBIhuX"
      },
      "source": [
        "In this lab, we will be taking our first look at developing our own *neural networks* (NN) with [PyTorch](https://pytorch.org/), probably the most popular machine learning library for working with NNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWQyyScGIhuY"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cj0g_bBIhuZ"
      },
      "source": [
        "### An Intuitive Intro to Neural Nets\n",
        "\n",
        "In today's lab, we're going to start with an intuitive exercise on the Titanic dataset using Logistic Regression and a simple Neural Network before moving onto some more complex stuff. Let's start by loading our dataset.\n",
        "\n",
        "Remember, the Titanic data is stored in a CSV file (located in the 'data' directory of your root folder), so we need to use Pandas to load the data and then separate it into our X (features) and y (target). We also need to: i) drop unimportant columns, and ii) impute missing values.\n",
        "\n",
        "We're going to do this all in the next cell - refer to the decision tree lab from yesterday for the detailed steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYb9noojIhua"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "data = fetch_openml(\"titanic\", version=1, as_frame=True, parser='auto').frame\n",
        "data.survived = pd.to_numeric(data['survived'])\n",
        "data.drop(['boat', 'body', 'home.dest'], axis=1, inplace=True)\n",
        "data = data.drop(['name', 'ticket', 'cabin', 'embarked'], axis=1) # remove unimportant columns\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(data['sex'])\n",
        "data['sex'] = le.transform(data['sex'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdHK09KWIhub"
      },
      "source": [
        "Next, as we have become accustomed to doing, we will split the dataset into a training set (where we will do our cross validation) and a test set (our hold-out data). We've done this a few times during the labs, so hopefully you're getting used to the process!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHYYoq9uIhuc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target_data = data[\"survived\"]\n",
        "feature_data = data.iloc[:, data.columns != \"survived\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UvxOkuoIhud"
      },
      "source": [
        "Now, we're ready to try out some models on our training data (you haven't seen anything new yet!). Since we're solving a binary classification problem (i.e., predicting a 0 or 1 target), we want to design classifiers. So far in the course we've covered the following simple classifiers: **k-nearest neighbors**, **decision trees**, and **logistic regression**.\n",
        "\n",
        "In this exercise, we're going to fit a logistic regression to our data and then design a neural network architecture that behaves exactly like a logistic regression and validate that we get the same result.\n",
        "\n",
        "##### Recap: Logistic Regression\n",
        "\n",
        "Logistic regression models are linear models similar to linear regression models. Hopefully you somewhat remember them from lecture. Let's review them, starting with the linear regression equation:\n",
        "\n",
        "\n",
        "<center>$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n$</center>\n",
        "\n",
        "Where $\\hat{y}$ is our prediction, $\\beta$ is our vector of coefficients (the things we learn), and $x$ is our feature vector. The linear regression equation defines a line in $n$ dimensional space. The problem with linear regression is that it doesn't really perform well on classification tasks. Consider the following example:\n",
        "\n",
        "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/linear-classification.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "The green line represents our trained linear regression model. Our feature is the size of a tumor, and our target is whether it is malignant or not (0 or 1). As we can see, even though our model is trained to the data to minimize error, for a lot of the values of tumor size it is going to give us a weird result (e.g., for some really small tumors, the prediction would be a negative value!).\n",
        "\n",
        "To resolve this, we use the *logistic function* (also called the *sigmoid* function) to 'squish' our linear model to be bounded by 0 and 1. The logistic (sigmoid) function is $\\frac{1}{1+e^{-x}}$, and thus our logistic regression equation becomes:\n",
        "\n",
        "<center>$\\large\\hat{y} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n)}}$</center>\n",
        "\n",
        "In this equation, the values for $\\hat{y}$ can never be below 0, and can never exceed 1 even for the most extreme feature values $x$.\n",
        "\n",
        "---\n",
        "\n",
        "**YOUR TURN:**\n",
        "* Assuming you've trained a nice logistic regression model to the below data (see Figure), what might the model fit look like (i.e., what will the line look like)? ____________________________________\n",
        "* For new data samples with features $x$, how would you convert the output of the logistic regression, $\\hat{y}$, into a classification (0 or 1)? ______________________________\n",
        "\n",
        "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/logistic-classification.jpg?raw=1\" width=\"400\"/>\n",
        "\n",
        "---\n",
        "\n",
        "OK, cool! So a quick review of logistic regression. Let's use scikit-learn to fit a logistic regression model to our training set and then predict on our test set (we won't do cross validation this time). Remember, when we used decision trees and tree ensembles, our cross validation accuracy was somewhere from 75-80%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7iInLxCIhue"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "## convert X_train and X_test to np arrays\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "\n",
        "n_features = X_train.shape[1] # number of features\n",
        "\n",
        "# impute Missing Values\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # Imputing missing values (training set)\n",
        "imp.fit(X_train)\n",
        "X_train = imp.transform(X_train)\n",
        "X_test = imp.transform(X_test) # Imputing missing values (test set)\n",
        "\n",
        "# Create and fit the model\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "predictions = logreg.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print (f\"Logistic accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9iX-X4Ihuf"
      },
      "source": [
        "And so our logistic regression gives us similar performance to the tree methods we explored yesterday.\n",
        "\n",
        "---\n",
        "\n",
        "**YOUR TURN:**\n",
        "* The default regularization parameter for sklearn's logistic regression is L2 (or ridge regression); can you figure out how to change it to L1 (LASSO)? _Note: you'll have to look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)!_ ______________________\n",
        "* What is the mean accuracy of an L1 regularized Logistic regression model on the training set? ______________________\n",
        "\n",
        "OK - time for the good stuff!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Neural Networks\n",
        "\n",
        "A *neural network* (NN) is a type of machine learning model that, like linear or logistic regression, takes a feature vector, X, as input and predicts a target, y. The way it does this is a little bit different, however. A typical NN architecture consists of: an input layer, hidden layers, and an output layer. Each layer consists of a set of nodes (neurons) connected by edges (outputs). Let's look at the figure below:\n",
        "\n",
        "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/nn.jpeg?raw=1\" width=\"400\"/>\n",
        "\n",
        "**Input layer**: This is a passive layer that simply takes in your feature data and outputs it to the hidden layers. You can think of each input layer neuron as being associated with a feature in your feature set.\n",
        "\n",
        "**Hidden layer**: This is where the magic happens. The original features, as received by the input layer, go through a series of transformations within the hidden layer. You can think of each node (neuron) within the hidden layer as a highly transformed feature.\n",
        "\n",
        "**Output layer**: This is where we get our final result, the 0 or 1 prediction.\n",
        "\n",
        "### Zooming In\n",
        "\n",
        "Let's take a look at what is happening at any given node (neuron) within the hidden layer. Take a look at the following image of a neuron within an NN:\n",
        "\n",
        "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/neuron.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Every neuron has some inputs ($x_1, x_2, \\dots, x_n$) with input weights ($w_1, w_2, \\dots, w_n$) and an output, $Y$. The neuron itself applies a transformation, $f$, known as the *activation function*, to the linear combination of its inputs and input weights. The value $b$ is a constant weight called the bias.\n",
        "\n",
        "There are many different types of activation functions, but the popular ones are the *sigmoid*, *tanh*, and *ReLU* activation functions. Yes, you heard correctly: the sigmoid function is a popular activation function! (This should be reminding you of the logistic regression model we discussed above).\n",
        "\n",
        "---\n",
        "\n",
        "**YOUR TURN:**\n",
        "* If you were to develop a simple neural network architecture that was equivalent to a logistic regression model for the Titanic data, how would you do it? Get a pen and paper and draw it out. Make sure to specify: the input layer, the hidden layer(s), the output layer, the activation function(s), the weights, and the biases.\n",
        "* How many hidden layers does your NN have? What type of activation function, $f$, does it use? _________________\n",
        "* Say you wanted to add another layer to your NN architecture with 3x neurons, what would your new architecture look like? _________________\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1McUuCs149Dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Intro to  PyTorch\n",
        "\n",
        "OK, so now that we've made the connection between NNs and Logistic Regression, let's code up our little NN in PyTorch and use it to predict survivorship on the Titanic dataset.\n",
        "\n",
        "First, **tensors** are the fundamental data type of PyTorch. Each tensor is effectively a multi-dimensional array, just like a numpy array. The primary difference is that tensors have been setup in such a way to enhance the NN training process. We can thing of them as \"upgraded\" numpy arrays.\n",
        "\n",
        "Let's load our X and y training data into tensors:"
      ],
      "metadata": {
        "id": "sQ0lCE3v5Dev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxobcMqFIhug"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "X_train_tensor = torch.Tensor(X_train).float()\n",
        "y_train_tensor = torch.Tensor(y_train.values) # we can only create a tensor from np arrays\n",
        "\n",
        "X_test_tensor = torch.Tensor(X_test).float()\n",
        "y_test_tensor = torch.Tensor(y_test.values).float()\n",
        "# we call .float() to convert every tensor to use floating point numbers since we need that for later.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
        "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
        "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n"
      ],
      "metadata": {
        "id": "Ge4UKjvvuNy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up our dataset to be *iterable* such that we can train our neural network in *batches*. A batch is a subset of the total data such that if we combined them all, we'd get the whole dataset. Batching is done to speed up the training process and reduce memory requirements.\n",
        "In Pytorch, this is done in two steps:\n",
        "1. creating a `Dataset` (or `TensorDataset`). This organizes the data and tells Pytorch that this is the data we want to work with.\n",
        "2. Setting up a `DataLoader`. The DataLoader takes a `Dataset` and automatically creates batches for by sampling the dataset.\n"
      ],
      "metadata": {
        "id": "msUbTuj3AV1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "id": "nvo2kpTnAS7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**YOUR TURN:**\n",
        "* If we select a batch size of 256, how many batches of training data will be generated?________________\n",
        "---\n"
      ],
      "metadata": {
        "id": "eQTqB-JABD20"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDsVueiIhuh"
      },
      "source": [
        "Next, we will actually define our logistic regression network model class.\n",
        "\n",
        "In Pytorch\n",
        "The below function, `LogisticRegression`, applies a sigmoid transformation to the output, as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnbPW--OIhuh"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module): # every pytorch network needs to inherit torch.nn.Module\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        # the __init__ class is used to define the components of our network\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim) # weights\n",
        "        self.sigmoid = torch.nn.Sigmoid() # sigmoid function\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x) # pass the data into the self.linear function (weighted sum of features)\n",
        "        outputs = self.sigmoid(logits) # apply sigmoid\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBNTYMbRIhuh"
      },
      "source": [
        "Next, we identify the dimensions of our problem: 6 x 1 (6 features and 2 target classes: 0 or 1), initialize our model with those dimensions and then specify the loss function ([cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)) and optimization technique ([stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAfHj63fIhui"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(input_dim=n_features, output_dim=1) # we are predicting the probability of a point being 1=\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ltyMFvsIhuj"
      },
      "source": [
        "Next, we train Logistic Regression and test its performance on the test set every 4000 iterations. We use 20,000 *epochs* and print our accuracy values every 4000 iterations. An epoch is when the entire dataset has passed through the network. An iteration is when a single forward/backward pass of the network over a batch of data is done.\n",
        "\n",
        "---\n",
        "\n",
        "**YOUR TURN:**\n",
        "* If our batch size is 256 and we elect to do 20,000 epochs (i.e., the network sees the entire dataset 20,000 times), how many iterations (forward/backward passes of the data) will our network see? _______________________\n",
        "---\n",
        "\n",
        "Run the below code and check out the incremental accuracy improvement output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2lxP28lIhuj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "iterations = 0\n",
        "epochs = 20000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (features, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # delete gradients\n",
        "        outputs = model(features) # make a prediction\n",
        "        loss = criterion(torch.squeeze(outputs), target) # calculate the error\n",
        "        loss.backward() # calculate gradients w/ respect to the loss\n",
        "        optimizer.step() # update the model parameters\n",
        "\n",
        "        iterations += 1\n",
        "        if iterations % 1000 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X_test_tensor)\n",
        "                predicted = torch.round(outputs.data).numpy()\n",
        "            model.train()\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            print (f\"Loss: {loss.item():.2f} | Accuracy: {accuracy:.2f} | Progress: {epoch/epochs*100:.2f}%\\t| Iteration: {iterations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuXmjwuMIhuk"
      },
      "source": [
        "Hopefully you got an accuracy of around ~77%-78%. What you'll notice is that is similar accuracy we got from sklearn's built-in logistic regression function from earlier in the lab. There are differences in the implementation that account for this discrepancy, notably sklearn used l2 regularization, and an optimizer called LBFGS instead of SGD that we learned and used.\n",
        "\n",
        "Let's take a look at the trained model parameters using the `model.parameters()` function within PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "535umxhZIhuk"
      },
      "outputs": [],
      "source": [
        "params = list(model.parameters())\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkfrWNPGIhuk"
      },
      "source": [
        "Hm, well this is interesting! We can see that our model consists of two tensors: the first has (1,6), and the second has dimension (1). Refer back to how you drew what you thought this NN architecture would look like.\n",
        "\n",
        "---\n",
        "**YOU TURN:**\n",
        "* What do you think these values represent? ____________________________\n",
        "* How many hidden layers does the architecture have? ______________________________\n",
        "* Draw the architecture and label (some of) the weights (trained parameters). ______________________________\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeZvj7ekIhul"
      },
      "source": [
        "You'll notice that this took considerably longer to train than scikit-learn's logistic regression: this is because PyTorch is set-up to be more flexible and train architectures much more complex than a simple single neuron network. Scikit-learn's implementation of logistic regression is highly optimized.\n",
        "\n",
        "---\n",
        "**YOUR TURN:**\n",
        "* How many epochs would you need to increase the process to 200,000 iterations? ______________________\n",
        "* Does increasing to 200,000 iterations improve your test set accuracy? ______________________\n",
        "* Compare the predictions of your logistic regression from scikit-learn and your network developed with PyTorch. Are all the predictions the same? How many predictions are pair-wise different? ______________________\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEscia6QIhul"
      },
      "source": [
        "Congratulations! You've completed an introduction to neural networks and PyTorch. If you want to explore more sophisicated architectures and applications, check out designing a PyTorch neural network to properly classify digit images here:\n",
        "\n",
        "https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
        "\n",
        "Other than that, you're done the lab!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}