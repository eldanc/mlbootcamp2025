{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2025/blob/main/lab_5_2_llms.ipynb)\n",
        "\n",
        "# UofT FASE ML Bootcamp\n",
        "#### Friday June 13, 2025\n",
        "#### Sentiment Classification and Prompting with Transformers - Lab 2, Day 5\n",
        "#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya\n",
        "##### Lab author: Hriday Chheda, Eldan Cohen, edited by Nakul Upadhya\n",
        "\n",
        "In this lab, you will focus on different approaches for developing a classifier using pre-trained Transformer models.\n",
        "\n",
        "In particular, you will focus on:\n",
        "1. Extracting pre-trained text embedding and then training a separate classifier to predict sentiment\n",
        "2. In-context zero-shot and few-shot learning in LLMs\n",
        "\n",
        "In this lab, you will be using the popular [HuggingFace's Transformers library](https://huggingface.co/docs/transformers/en/index).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cBTCeNVlILql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by installing and importing the required libraries:"
      ],
      "metadata": {
        "id": "xI23hV99KRlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfhlIAWKH78o"
      },
      "outputs": [],
      "source": [
        "! pip install -U datasets\n",
        "! pip install -q transformers[torch]\n",
        "! pip install -q evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "3zv7WBF6K-H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Sentiment Classification using Feature Extraction from BERT\n",
        "\n",
        "We will be using a training and evaluation dataset containing financial tweets and a label indicating whether they are Bearish (believing prices will drop), Bullish (believing prices will rise), or Neutral.\n",
        "In this task, you will use a pre-trained BERT model without fine-tuning and use the representation obtained from this pre-trained BERT as features for a separate classifier."
      ],
      "metadata": {
        "id": "CIAbQ7oAVDHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First you will load the dataset by running the following chunk of code"
      ],
      "metadata": {
        "id": "5dZ_WshIZ8b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\", split=\"validation\")\n",
        "print(\"Example tweet:\", ds[0])\n",
        "\n",
        "#Clean the tweets to remove URLs\n",
        "ds = ds.map(lambda x: {\"text\": re.sub(r'http\\S+', '', x[\"text\"]).strip(), \"label\": x[\"label\"]})\n",
        "print(\"Example tweet:\", ds[0])"
      ],
      "metadata": {
        "id": "ZNF8EM8VLehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tweet has a label:\n",
        "- 0: Bearish (believing prices will drop)\n",
        "- 1: Bullish (believing prices will rise)\n",
        "- 2: Neutral"
      ],
      "metadata": {
        "id": "u0YJyMKNacrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0] # 'label' field"
      ],
      "metadata": {
        "id": "LL1BbYFnaXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting to training and evaluation sets:"
      ],
      "metadata": {
        "id": "5QFlulOLapaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = ds.train_test_split(test_size=0.1)\n",
        "training_set = split[\"train\"]\n",
        "eval_set = split[\"test\"]"
      ],
      "metadata": {
        "id": "IGIgHSLYal2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the data setup we focus on extracting representation from a pre-trained LLM. In this case we choose the BERT model (specifically, \"bert-base-uncased\"). Note, our goal is to create a classifier model that can predict the label for a given tweet.\n",
        "The idea is to first, extract the representation of the tweets from a pre-trained BERT model, then, use the extracted representations as features along with the given labels to train a SVC (support vector classifier) classifier to predict the label for a given tweet based on it's BERT representation."
      ],
      "metadata": {
        "id": "lBgBwqCoawzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Remember, we need to tokenize the tweets to input them to a LLM\n",
        "# Here we load the appropriate tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "U-S8-bJ6bLF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take one example from our training set and tokenize it:"
      ],
      "metadata": {
        "id": "wEoVwT8DdjrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_tweet = training_set[0][\"text\"]\n",
        "print(example_tweet)\n",
        "# tokenize test and print the ID for each token:\n",
        "tokenized_example = tokenizer(example_tweet)\n",
        "print(tokenized_example[\"input_ids\"])"
      ],
      "metadata": {
        "id": "gNQPR3ijdjTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the tokenizer to translate back each token ID to the vocabulary word:"
      ],
      "metadata": {
        "id": "67l-821Sd7Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([tokenizer.decode(token_id) for token_id in tokenized_example[\"input_ids\"]])"
      ],
      "metadata": {
        "id": "OMgxHHcBbOKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will set up a feature extraction pipeline that will take a text and generate the features for this text using the BERT model and tokenizer we loaded."
      ],
      "metadata": {
        "id": "ak1pVAVxeFw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "CTrMjuJ6d-m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the pipeline to obtain representation for these three examples:\n",
        "example_result = feature_extractor(example_tweet, return_tensors = \"pt\")\n",
        "print(example_result.shape)"
      ],
      "metadata": {
        "id": "bqtKpV6meMvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each tweet we will have a [1, LENGTH, 768] feature vector (LENGTH is the number of tokens, while 768 is the latent dimension). We can summarize the features for each data point using mean pooling across the token contextual representations to a fixed size representation (768 dimensions for this BERT model):"
      ],
      "metadata": {
        "id": "KlvMtePxeVB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This takes the mean of the feature across all the tokens\n",
        "example_result[0].numpy().mean(axis=0).shape"
      ],
      "metadata": {
        "id": "jOGmhKxSePhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1\n",
        "\n",
        "---\n",
        "\n",
        "**Your Turn**\n",
        "\n",
        "Using the example from above, finish the function that will extract the fixed size contextual embeddings of both the training set and the evaluation set using BERT\n",
        "\n"
      ],
      "metadata": {
        "id": "LWL2lyoLhBe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training set embeddings\n",
        "\n",
        "def extract_embeddings(dataset):\n",
        "  X= np.zeros((dataset.num_rows, 768))\n",
        "  y = np.zeros(dataset.num_rows)\n",
        "  for i in tqdm(range(dataset.num_rows)):\n",
        "    example_tweet = dataset[i][\"text\"]\n",
        "    example_result = feature_extractor(example_tweet, return_tensors = \"pt\")\n",
        "    example_summarized = example_result[0].numpy().mean(axis=0)\n",
        "    X[i] = example_summarized\n",
        "    y[i] = dataset[i][\"label\"]\n",
        "  return X, y\n"
      ],
      "metadata": {
        "id": "NzIM0MRPefBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We will then use the function you filled out to populate X and y datasets."
      ],
      "metadata": {
        "id": "3Cb3j682VtA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = extract_embeddings(training_set)\n",
        "X_eval, y_eval = extract_embeddings(eval_set)"
      ],
      "metadata": {
        "id": "aKWJAFtZX2yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2\n",
        "\n",
        "Now we can use the extracted embeddings to train a classifier model. Here we choose the SVC classifier. Note: typically we would do hyper-parameter tuning for the classifier using a held-out validation set. For simplicity, just use the default hyper-parameters."
      ],
      "metadata": {
        "id": "o5cZi-3SjQhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SVC model\n",
        "svc_model = SVC()"
      ],
      "metadata": {
        "id": "ur4scj7BoLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Your Turn**\n",
        "\n",
        "Train the SVC on X_train and make predictions on both the training and testing set\n"
      ],
      "metadata": {
        "id": "85rhCgekYGBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the SVC\n",
        "\n",
        "\n",
        "## Evaluate the results\n",
        "train_predictions = # TODO: Call the predict method of the svc_model on the training features\n",
        "train_accuracy = accuracy_score(train_predictions, y_train)\n",
        "print(f\"Training accuracy is: {train_accuracy}\")\n",
        "\n",
        "\n",
        "eval_predictions = # TODO: Call the predict method of the svc_model on the eval features\n",
        "eval_accuracy = accuracy_score(eval_predictions, y_eval)\n",
        "print(f\"Evaluation accuracy is: {eval_accuracy}\")"
      ],
      "metadata": {
        "id": "rpOtdS-PoJHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "sSZ4tiQcezON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. In-context Zero-shot Learning\n",
        "\n",
        "Finally, we investigate in-context zero-shot learning using an instruction-trained LLM. Specifically, we use [Flan T5](https://huggingface.co/google/flan-t5-base) as it is relatively small and does not require significant resources to run.\n",
        "\n",
        "In contextual allows a model to perform tasks without prior specific training by using context and general knowledge. For instance, if the model has never been trained on the specific phrase \"The movie was a rollercoaster of emotions,\" it can still determine that the sentiment is positive by understanding the context of the words \"rollercoaster\" and \"emotions\" in relation to typical movie reviews. This capability allows the model to accurately assess sentiments in novel sentences without needing explicit prior examples.\n",
        "\n",
        "In this section you will work on creating prompts from In-context zero shot learning for the sentiment of movie reviews"
      ],
      "metadata": {
        "id": "C9-rPZP6t8cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "iWzivWe7uJjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Flan T5 (base) model and tokenizer:"
      ],
      "metadata": {
        "id": "DKYHxmYDuoxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "gtMvcWCtukO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first look at a simple zero-shot machine translation task:"
      ],
      "metadata": {
        "id": "fqpZ3l0qwyiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: How old are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "jce2lQA9u9Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to classify the sentiment of movie reviews. Here is a simple prompting template for classifying movie reviews:"
      ],
      "metadata": {
        "id": "41o7rnPixdob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\\\"{review_text}\\\". \\nIs it good?\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "l32XISk1vEVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can prompt the model using this prompt tempelate and an example review"
      ],
      "metadata": {
        "id": "gdX99PCv2Jsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_review = \"This movie was so enjoyable and I recommend it to everyone.\"\n",
        "\n",
        "input_text = prompt_template.format(review_text=example_review)\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids, max_new_tokens=10)\n",
        "print(tokenizer.decode(outputs[0], max_length=10))"
      ],
      "metadata": {
        "id": "dskle4rdyAdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_review = \"This movie is very well made. Still, I did not enjoy it and cannot recommend it to anyone.\"\n",
        "\n",
        "input_text = prompt_template.format(review_text=example_review)\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids, max_length=10)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "fORhsXq400bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the IMDB movie reviews dataset"
      ],
      "metadata": {
        "id": "P8rbw0eD2QD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "split = ds.train_test_split(test_size=0.002, seed=42)\n",
        "training_set = split[\"train\"]\n",
        "eval_set = split[\"test\"]"
      ],
      "metadata": {
        "id": "TralKmyl2bu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record includes a review text and a label: 0 for negative, 1 for positive. For example:"
      ],
      "metadata": {
        "id": "Vw8hW3Ru3Kh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set[0]"
      ],
      "metadata": {
        "id": "ks-qN5Fo3MGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the small evaluation dataset of 50 reviews to evaluate the model using the prompt above"
      ],
      "metadata": {
        "id": "8EP5C0Lu2y75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = eval_set[\"text\"]\n",
        "labels = eval_set[\"label\"]"
      ],
      "metadata": {
        "id": "sQlXwID52l41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for idx, text in tqdm(enumerate(texts)):\n",
        "  input_text = prompt_template.format(review_text=text)\n",
        "  input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "  outputs = model.generate(input_ids, max_length=10)\n",
        "  output_text = tokenizer.decode(outputs[0])\n",
        "  if \"no\" in output_text:\n",
        "    predictions.append(0)\n",
        "  else:\n",
        "    predictions.append(1)\n",
        "\n",
        "print(\"\\npredictions:\", predictions)"
      ],
      "metadata": {
        "id": "2HGMxFQf3oD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure accuracy:"
      ],
      "metadata": {
        "id": "RCDL_kjt8bvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(predictions, labels)"
      ],
      "metadata": {
        "id": "uWtCX1uh3yD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Your Turn**\n",
        "\n",
        "Investigate the reviews for which the model did not make correct predictions according to the provided labels. What is the reason? Did the model make a mistake? (it is also possible that you agree with the model and the label, in fact, does not seem to be correct - explain if this is the case)\n",
        "\n",
        "Answer: __________"
      ],
      "metadata": {
        "id": "662F9dFl8PXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the performance using this prompt is better than random guessing, can you try other prompts to improve the accuracy? (For example: Specifically ask whether the review is positive or negative, etc). Be as creative as you wish and report results using 2 different prompts."
      ],
      "metadata": {
        "id": "ObBNVP3c8vqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Try different prompts to classify the reviews"
      ],
      "metadata": {
        "id": "pzVeWMm19z2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "p6tJgE9BYgEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Few Shot Prompting\n",
        "\n",
        "Few-shot prompting enables large language models to perform better on complex tasks by providing demonstrations. While zero-shot capabilities have shown remarkable results, few-shot prompting has emerged as a more effective way to tackle complex tasks by utilizing different numbers of demonstrations, such as 1-shot, 3-shot, 5-shot, and so on.\n",
        "\n",
        "We present some examples that use few shot prompting. We use the GPT neo model with 1.3B models trained by EleutherAI which is LLM that replicates GPT-3 architecture and is free to use. You can read more about this model [here](https://huggingface.co/EleutherAI/gpt-neo-1.3B)"
      ],
      "metadata": {
        "id": "EroKH2fAjyRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "8mZXtwL0mYXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ],
      "metadata": {
        "id": "2Nt5XSrt6A3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example 1: Translation\n",
        "To demonstrate few-shot prompting, consider the following example in which the task is to translate \"Thank you\" to French. The expected answer is \"merci\". First we can try prompting the model with a direct zero-shot prompts as below:"
      ],
      "metadata": {
        "id": "Dbowmjaxo9Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Translate \"Thank you\" to French\"\"\"\n",
        "output = generator(input_text, max_new_tokens=10, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bRDPmqSxOy52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"What is \"thank you\" in French?\"\"\"\n",
        "output = generator(input_text, max_new_tokens=20, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "v6FnI3BUPpyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see these direct prompts are unable to provide the desired result. Maybe we can try giving the model one example and try a one-shot prompt as below?\n"
      ],
      "metadata": {
        "id": "GIdPJ5-3P8_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "Example:\n",
        "English: \"Good morning.\"\n",
        "French: \"Bonjour.\"\n",
        "\n",
        "Now you try:\n",
        "English: \"Thank you.\"\n",
        "French:\"\"\"\n",
        "output = generator(input_text, max_new_tokens=4, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cr7c6VvI2mDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example 2: Named Entity Recognition\n",
        "Here the task is to identify and label the named entities in the input sentence.\n",
        "\n",
        "For example, Sentence: \"Barack Obama was born in Hawaii.\"\n",
        "\n",
        "Answer: Barack Obama is a person and Hawaii is a location."
      ],
      "metadata": {
        "id": "yDiG1flOQ1y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Task: Identify and label the named entities in the following sentence.\n",
        "Barack Obama was born in Hawaii.\n",
        "\"\"\"\n",
        "output = generator(input_text, max_new_tokens=20, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rVTSF-_iR07E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Example:\n",
        "Sentence: \"Barack Obama was born in Hawaii.\"\n",
        "Entities: [Barack Obama: PERSON, Hawaii: LOCATION]\n",
        "\n",
        "Now you try:\n",
        "Sentence: \"Justin Trudeau is the prime minister of Canada.\"\n",
        "Entities:\"\"\"\n",
        "output = generator(input_text, max_new_tokens=11, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "1CBpwoRwnBrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example 3: Question Answering (Your Turn)\n",
        "\n",
        "The task in this example is to test the comprehension abilities of the model. Given an input passage is the model able to answer a question based on the passage correctly.\n",
        "\n",
        "For example a passage could be, \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\"\n",
        "A suitable question based on the passage: \"What did Marie Curie win?\"\n",
        "\n",
        "1. Use few shot learning prompt to answer the following questions:\n",
        "\"Thomas Edison was an American inventor who developed many devices including the phonograph and the electric light bulb.\"\n",
        "\"What did Thomas Edison develop?\"\n",
        "\n",
        "2. \"Mount Everest is the highest mountain in the world, located in the Himalayas on the border between Nepal and China.\"\n",
        "\"Where is Mount Everest located?\"\n"
      ],
      "metadata": {
        "id": "BLhyDLIsoyJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Fill in your one-shot prompt for passage 1 in the input_text below\n",
        "input_text = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "output = generator(input_text, max_new_tokens=10, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "iSIy2jAqSoT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Fill in your one-shot prompt for passage 2 in the input_text below\n",
        "input_text = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "output = generator(input_text, max_new_tokens=10, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rBgl6oRCrX3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limitations of few shot prompting\n",
        "\n",
        "Consider the task of asking a large language model to solve math word problems for example:\n",
        "Sarah has 12 apples. She gives 3 apples to each of her 2 friends. How many apples does she have left?\n",
        "Of course we know the answer is 6. Is the LLM able to get the answer?\n"
      ],
      "metadata": {
        "id": "Vzvl3PdyvKr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Sarah has 12 apples. She gives 3 apples to each of her 2 friends.\n",
        "How many apples does she have left? (output a number)\n",
        "\"\"\"\n",
        "output = generator(input_text, max_new_tokens=10, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "uhOsUxCEwGYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't expect the model to answer this question correctly but what if we use one shot prompting in this case?"
      ],
      "metadata": {
        "id": "FvoUUkDjwox7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Task: Solve the following math word problem step-by-step.\n",
        "\n",
        "Example 1:\n",
        "Problem: Sarah has 12 apples. She gives 3 apples to each of her 2 friends. How many apples does she have left?\n",
        "Answer: 6\n",
        "\n",
        "Now you try:\n",
        "Example 2:\n",
        "Problem: John has 5 packs of crayons. Each pack contains 8 crayons. He gives 15 crayons to his friends. How many crayons does he have now?\n",
        "Answer:\"\"\"\n",
        "output = generator(input_text, max_new_tokens=2, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "KBl0X52lw7eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the model is able to output some answer it is clearly not the correct answer. Few-shot prompting provides a model with a limited number of examples, which may not be sufficient for understanding the nuances of complex tasks. The model might struggle to generalize from these examples, especially when the tasks involve multiple steps or intricate logic."
      ],
      "metadata": {
        "id": "nfLH8irN0Gts"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B90EuEaxJMJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}