{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2025/blob/main/lab_5_1_words.ipynb)\n",
        "\n",
        "# UofT FASE ML Bootcamp\n",
        "#### Friday June 13, 2025\n",
        "####  Word Embeddings - Properties, Meaning and Training - Lab 1, Day 5\n",
        "#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya\n",
        "##### Lab author: Nakul Upadhya (Based on CARTE-DSI ML Bootcamp 2023 notebook by Prof. Jonathan Rose)\n",
        "\n",
        "This lab engages you in the properties, meaning, viewing and training of word embeddings (also called word vectors). The specific learning objectives in this assignment are:\n",
        "\n",
        "1.   To learn word embedding properties, and use them in simple ways.\n",
        "2.   (optional) To translate vectors into understandable categories of meaning\n",
        "3.   To understand how embeddings are created, using the Skip Gram method.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDdF-DpXWg1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting and Understanding Word Embedding/Vectors\n",
        "\n",
        "\n",
        "Word embeddings (also known as word vectors) are a way to encode the meaning of words into a set of numbers.\n",
        "\n",
        "These embeddings are created by training a neural network model using many examples of the use of language.  These examples could be the whole of Wikipedia or a large collection of news articles.\n",
        "\n",
        "To start, we will explore a set of word embeddings that someone else took the time and computational power to create. One of the most commonly-used pre-trained word embeddings are the **GloVe embeddings**."
      ],
      "metadata": {
        "id": "UMKnFM6Wkl46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Embeddings\n",
        "\n",
        "You can read about the GloVe embeddings here: https://nlp.stanford.edu/projects/glove/, and read the original paper describing how they work here: https://nlp.stanford.edu/pubs/glove.pdf.\n",
        "\n",
        "There are several variations of GloVe embeddings. They differ in the text used to train the embedding, and the *size* of the embeddings.\n",
        "\n",
        "Throughout this lab we'll use a package called `staticvectors`, a package for obtaining pre-trained static word embeddings that is part of the `huggingface` model family.\n",
        "\n",
        "We'll begin by loading a set of GloVe embeddings. The first time you run the code below, it will cause the download of a large file (862MB) containing the embeddings."
      ],
      "metadata": {
        "id": "rNg2XmmFlfuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "!pip install huggingface staticvectors\n",
        "import torch\n",
        "from staticvectors import StaticVectors\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PjRMuu2Fnb9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypuiFEIYWfkz"
      },
      "outputs": [],
      "source": [
        "glove = StaticVectors('neuml/glove-6B')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the loaded glove embeddings to look up the embeddings of individual words.\n",
        "For example, let's look at what the embedding of the word \"apple\" looks like:"
      ],
      "metadata": {
        "id": "NiKD_dpmtuWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Shape:\", glove.embeddings('apple').shape)\n",
        "print(glove.embeddings('apple'))"
      ],
      "metadata": {
        "id": "M4i4ocTQmIYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output above, the embedding of a given word is a numpy array with dimension `(50,)`. We don't know what the meaning of each number is, but we do know that there are properties of the embeddings that can be observed. For example, `distances between embeddings` are meaningful.\n",
        "\n",
        "## Measuring Distance\n",
        "\n",
        "Let's consider one specific metric of distance between two embedding vectors called the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
        "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
        "the Euclidean distance between $x$ and $y$: $\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
        "\n",
        "\n",
        "Here we define a function to calculate the norm between two vectors:\n"
      ],
      "metadata": {
        "id": "GB-dyAZjvdKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_euclidean_distance(x, y):\n",
        "  a = glove.embeddings(x) # get embeddings\n",
        "  b = glove.embeddings(y)\n",
        "  return np.linalg.norm(a-b) # use np.linalg to calculate the norm"
      ],
      "metadata": {
        "id": "4hvhg6S-vNjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use this function to get distances between words:"
      ],
      "metadata": {
        "id": "k2BAA1yY67_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_euclidean_distance('apple', 'banana')"
      ],
      "metadata": {
        "id": "_897mUSTwAwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_euclidean_distance('good', 'bad')"
      ],
      "metadata": {
        "id": "6Z-zVh8CwMtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_euclidean_distance('good', 'water')"
      ],
      "metadata": {
        "id": "kgfIIcTxwO99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_euclidean_distance('good', 'well')"
      ],
      "metadata": {
        "id": "dZAQ64sNwRfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_euclidean_distance('good', 'perfect')"
      ],
      "metadata": {
        "id": "mEdfGmh7wmN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "An alternative and more commonly-used measure of distance is the **Cosine Similarity**. The cosine similarity measures the *angle* between two vectors, and has the property that it only considers the *direction* of the vectors, not their the magnitudes. It is computed as follows for two vectors A and B:\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1hSaQRBjH828lx1xozJCA4F0ZhiX2S0Xt)\n",
        "\n",
        "Lets create a new function to measure the cosine similarity between two words:\n"
      ],
      "metadata": {
        "id": "FwCYQe_XxLik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_cosine_distance(x, y):\n",
        "  a = glove.embeddings(x) # get embeddings\n",
        "  b = glove.embeddings(y)\n",
        "  norm_a = np.linalg.norm(a)\n",
        "  norm_b = np.linalg.norm(b)\n",
        "  return np.dot(a, b) / (norm_a * norm_b)"
      ],
      "metadata": {
        "id": "cd8iVpD4wp28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cosine similarity is actually a *similarity* measure rather than a *distance* measure, and gives a result between -1 and 1. Thus, the larger the similarity, (closer to 1) the \"closer in meaning\" the word embeddings are to each other."
      ],
      "metadata": {
        "id": "-kAON3H92whE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_cosine_distance('cat', 'dog')"
      ],
      "metadata": {
        "id": "CT_gUzm34S2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cosine_distance('good', 'bad')"
      ],
      "metadata": {
        "id": "2BsgHFVF4W9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cosine_distance('good', 'water')"
      ],
      "metadata": {
        "id": "wBpjBMU78mbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cosine_distance('good', 'perfect')"
      ],
      "metadata": {
        "id": "MJnJkqaH8zRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cosine_distance('watermelon', 'airplane')"
      ],
      "metadata": {
        "id": "Wq0HJ62Z81zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Similarity\n",
        "\n",
        "Now that we have notions of distance and similarity in our embedding space, we can talk about words that are \"close\" to each other in the embedding space. For now, let's use Euclidean distances to look at how close various words are to the word \"cat\"."
      ],
      "metadata": {
        "id": "YR-6jQto5b8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
        "for w in other:\n",
        "    dist = word_euclidean_distance(word, w)\n",
        "    print(w, \"\\t%5.2f\" % float(dist))"
      ],
      "metadata": {
        "id": "5wg_H1Bk48ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the same thing with cosine similarity:"
      ],
      "metadata": {
        "id": "NT2_dyt-6LZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
        "for w in other:\n",
        "    dist = word_cosine_distance(word, w)\n",
        "    print(w, \"\\t%5.2f\" % float(dist))"
      ],
      "metadata": {
        "id": "jwBTNAz96AFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look through the entire **vocabulary** for words that are closest to a point in the embedding space -- for example, we can look for words that are closest to another word such as \"cat\"."
      ],
      "metadata": {
        "id": "QumucClS6b8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_closest_words(glove_vector, n=5):\n",
        "  tokens = glove.tokens\n",
        "  distances = [None] * len(tokens)\n",
        "  for i, token in enumerate(tokens):\n",
        "      distance = np.linalg.norm(glove.embeddings(token) - glove_vector)\n",
        "      distances[i] = {\n",
        "          'token': token,\n",
        "          'distance': distance\n",
        "      }\n",
        "  sorted_distances = sorted(distances, key=lambda x: x['distance'])\n",
        "  df = pd.DataFrame(sorted_distances[1:n+1]) # ignore the word itself\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "6MxGErGA6O9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings(\"cat\"), n=10)"
      ],
      "metadata": {
        "id": "oL4H4tjsBqMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings(\"dog\"), n=10)"
      ],
      "metadata": {
        "id": "K6JQA84I6j4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings(\"doctor\"), n=10)"
      ],
      "metadata": {
        "id": "mmQicawa8IL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Your Turn**\n",
        "\n",
        "Try searching for similar words for words of your choice here:\n"
      ],
      "metadata": {
        "id": "73KzRzLeDy7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can also try printing closest words to any other words of your choice here:\n",
        "\n"
      ],
      "metadata": {
        "id": "ThHIes6D8dJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "pIyf-7fBD428"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also look at which words are closest to the midpoints of two words:"
      ],
      "metadata": {
        "id": "yuDTCwCI90qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words((glove.embeddings('happy') + glove.embeddings('sad')) / 2)"
      ],
      "metadata": {
        "id": "PGusXTUB91R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words((glove.embeddings('doctor') + glove.embeddings('engineer')) / 2)\n"
      ],
      "metadata": {
        "id": "BV-ISeLA96js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Analogies\n",
        "\n",
        "One surprising aspect of word embeddings is that the *directions* in the embedding space can be meaningful. For example, some analogy-like relationships like this tend to hold:\n",
        "\n",
        "$$ king - man + woman \\approx queen $$\n",
        "\n",
        "Analogies show us how relationships between pairs of words that is captured in the learned vectors"
      ],
      "metadata": {
        "id": "F3D4X_Q0_uv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings('king') - glove.embeddings('man') + glove.embeddings('woman'))"
      ],
      "metadata": {
        "id": "rq_MVbTd-cwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top result is a reasonable answer like \"queen\", and the other results include words like \"princess.\"\n",
        "\n",
        "We can also do the reverse!"
      ],
      "metadata": {
        "id": "SObT9dicM8R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings('queen') - glove.embeddings('woman') + glove.embeddings('man'))"
      ],
      "metadata": {
        "id": "HCOLWngBM-7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove.embeddings('king') - glove.embeddings('prince') + glove.embeddings('princess'))"
      ],
      "metadata": {
        "id": "5PbjuVO8M-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Your Turn**\n",
        "\n",
        "Consider now the word pair relationships given in Figure 1 below, which comes from Table 1 of the Mikolov [[link](https://arxiv.org/abs/1301.3781)] paper. Choose one of these relationships, but not one of the ones already shown above, and report which one you chose. Write and run code that will generate the second word given the first word. Generate 10 more examples of the same relationship from 10 other words, and comment on the quality of the results.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1O7Zizu63jj5aoZkGkK0sz93CZSEsBDuW)\n",
        "\n"
      ],
      "metadata": {
        "id": "CJjQLdz_OAuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Choose one of the relationships from the table above and generate 10 examples\n",
        "\n"
      ],
      "metadata": {
        "id": "XgvVPnEvNI5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "w5c61saiF_TS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training A Word Embedding Using the Skip-Gram Method on a Small Corpus\n",
        "\n",
        "So far in this notebook we've used the pre-trained GloVe embeddings. The lecture this morning described the Skip Gram method of training word embeddings. In this section you are going to review code to use that method to train a very small embedding, for a very small vocabulary on a very small corpus of text. The goal is to gain some insight into the general notion of how embeddings are produced. The corpus you are going to use is in the file SmallSimpleCorpus.txt, and was also shown in the lecture."
      ],
      "metadata": {
        "id": "moAuiKpZazXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy"
      ],
      "metadata": {
        "id": "whMJrpncbpAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "vPGnmHKPnvEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we read the file SmallSimpleCorpus.txt into our notebook and do some pre-processing to make the learning easier later. Specifically, we lemmatize the corpus,\n",
        "which means converting words to their root - for example the word “holds” becomes “hold”, whereas the word  “hold” itself stays the same.\n",
        "The `prepare_texts` function performs lemmatization using the [spaCy](https://spacy.io/models/en) library."
      ],
      "metadata": {
        "id": "tzqJDqfqn1uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/eldanc/mlbootcamp2025/refs/heads/main/SmallSimpleCorpus.txt\n",
        "\n",
        "with open('./SmallSimpleCorpus.txt', 'r') as file:\n",
        "    corpus = file.read()\n",
        "corpus"
      ],
      "metadata": {
        "id": "91-JsD1_oNex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def prepare_texts(corpus):\n",
        "    doc = nlp(corpus)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return lemmas\n",
        "#lematize the corpus and create the vocabulary\n",
        "lemmas = prepare_texts(corpus)\n",
        "vocab = set(lemmas)\n",
        "v2i = {v: i for i, v in enumerate(vocab)} # dictionary to lookup word to index\n",
        "i2v = {i: v for v, i in v2i.items()} # dictionary to lookup index to word\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ],
      "metadata": {
        "id": "MtjWNYL9KukH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `tokenize_and_preprocess_text` takes the lemmatized small corpus as input, along with `v2i` (which serves as a simple, lemma-based tokenizer) and a window size window. Its output should be the Skip Gram training dataset for this corpus: pairs of words in the corpus that “belong” together, in the Skip Gram sense.\n",
        "That is, for every word in the corpus a set of training examples are generated with that word serving as the (target) input to the predictor,\n",
        "and all the words that fit within a window of size window surrounding the word would be predicted to be in the “context” of the given word.\n",
        "The words are expressed as tokens (numbers)."
      ],
      "metadata": {
        "id": "66Ois3z7qS9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess the text\n",
        "def tokenize_and_preprocess_text(lemmas, v2i, window=3):\n",
        "    data = []\n",
        "    for i in range(len(lemmas)):\n",
        "        target = v2i[lemmas[i]]\n",
        "        context = []\n",
        "        for j in range(i - window // 2, i + window // 2 + 1):\n",
        "            if j != i and j >= 0 and j < len(lemmas):\n",
        "                context.append(v2i[lemmas[j]])\n",
        "        for c in context:\n",
        "            data.append((target, c))\n",
        "    return data"
      ],
      "metadata": {
        "id": "52WgB7prqPIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Skip gram dataset with window size of 5\n",
        "window_size = 5\n",
        "data = tokenize_and_preprocess_text(lemmas, v2i, window_size)\n",
        "print(data[:5])"
      ],
      "metadata": {
        "id": "KOZ_-mqYqQNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of this is `data`, a list that enumerates co-occurences of tokens together. The task is now to train a classification model that aims to \"predict\" the second token using the embedding from the first.\n",
        "\n",
        "\n",
        "Review the code in Word2vecModel. Part of this model ultimately provides the trained embeddings/vectors, and you can see these are defined and initialized to random numbers in the line `self.embedding = torch.nn.Parameter(torch.rand(\n",
        "            vocab_size, embedding_size))`"
      ],
      "metadata": {
        "id": "cyQIHdXssbI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Word2Vec model\n",
        "class Word2VecModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super(Word2VecModel, self).__init__()\n",
        "        self.embedding = torch.nn.Parameter(torch.rand(\n",
        "            vocab_size, embedding_size))\n",
        "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding[x]\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tUti3LGZsZLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocab size\n",
        "embedding_size = 2 # Size of the embedding vector\n",
        "# Initialize the Word2Vec model\n",
        "model = Word2VecModel(vocab_size, embedding_size)"
      ],
      "metadata": {
        "id": "1SrHFoHns34Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the code for training the model. It uses Cross Entropy loss function described in the lecture, a batch size of 4, a window size of 5, and 50 Epochs of training. It uses the Adam optimizer, and a learning rate of 0.001."
      ],
      "metadata": {
        "id": "Su1mTcsFtWKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "def train_word2vec(model, data, epochs=50, batch_size=4, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(data)\n",
        "        losses = []\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i+batch_size]\n",
        "            inputs, labels = zip(*batch)\n",
        "            inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {np.mean(losses):.4f}')\n",
        "\n",
        "train_word2vec(model, data)"
      ],
      "metadata": {
        "id": "tXQOkAgkgCs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below that displays each of the embeddings in a 2-dimensional plot using Matplotlib.\n"
      ],
      "metadata": {
        "id": "BcJMpAzrvC--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(model, i2v):\n",
        "    embeddings = model.embedding.data.numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, word in i2v.items():\n",
        "        x, y = embeddings[i]\n",
        "        plt.scatter(x, y)\n",
        "        plt.text(x + 0.02, y + 0.02, word, fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "plot_embeddings(model, i2v)"
      ],
      "metadata": {
        "id": "HjUiXq6chAnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Your Turn**\n",
        "\n",
        "* Look at the original corpus above. Do the results from the embedding make sense?\n",
        "\n",
        "* What could happen when the window size is too large?\n",
        "\n",
        "* At what value would window become too large for this corpus?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CRYB0G7tvSxL"
      }
    }
  ]
}